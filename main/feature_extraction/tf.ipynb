{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk \n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/media/mangaldeep/HDD2/workspace/MotionControl_MasterThesis')\n",
    "from main.extraction.physionet_MI import extractPhysionet\n",
    "from main.extraction.bci3_IVa import extractBCI3\n",
    "from data import brain_atlas  as bm\n",
    "plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physionet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = extractPhysionet(runs = [3], person_id = 3)\n",
    "# # raw.pick_channels(['C3','C4'])\n",
    "# raw.notch_filter(60)\n",
    "# raw = raw.filter(7,50,verbose = False).copy()\n",
    "\n",
    "# event_data = mne.events_from_annotations(raw)\n",
    "# event_marker, event_ids = event_data\n",
    "# event_ids = dict({'T1':2, 'T2':3})\n",
    "# epochs = mne.Epochs(raw, event_marker, event_ids, tmin=-2, tmax=4, preload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_file = '../preproc/Physionet_trial_epo.fif'\n",
    "# epochs = mne.read_epochs(epoch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = epochs.apply_proj().copy()\n",
    "# epochs = epochs.apply_baseline((-2.0,-0.2)).copy()\n",
    "# epochs.equalize_event_counts()\n",
    "# tmin, tmax = -0.5, 2\n",
    "# epochs.crop(tmin= tmin, tmax=tmax)\n",
    "# picks=['C1', 'C2','C3', 'C4', 'C5', 'C6', 'Cz']#,'T9','T10']\n",
    "# epochs.pick_channels(picks)\n",
    "# T1 = epochs['T1']\n",
    "# T2 = epochs['T2']\n",
    "# t1 = epochs['T1'].average()\n",
    "# t2 = epochs['T2'].average()\n",
    "# # mne.viz.plot_compare_evokeds([T1,-T2],picks= c);\n",
    "# classes = list(event_ids.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCI3 -IVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['-1.0', '0.0', '1.0']\n",
      "Not setting metadata\n",
      "Not setting metadata\n",
      "280 matching events found\n",
      "Setting baseline interval to [-0.5, 0.0] sec\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Loading data for 280 events and 151 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88047/3003246356.py:9: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw, event_marker, event_ids, tmin=-0.5, tmax=1, preload = True)\n"
     ]
    }
   ],
   "source": [
    "raw = extractBCI3(runs = 0, person_id = 3)\n",
    "raw = raw.filter(7,30,verbose = False).copy()\n",
    "\n",
    "event_data = mne.events_from_annotations(raw)\n",
    "event_marker, event_ids = event_data\n",
    "ff= event_marker[:,-1].argsort()\n",
    "event_marker = event_marker[ff,:]\n",
    "event_ids = dict(right = 1, foot = 3, test = 2)\n",
    "epochs = mne.Epochs(raw, event_marker, event_ids, tmin=-0.5, tmax=1, preload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../preproc/BCI3_ssp_car_ica_[3]_P3_epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 3 projection items:\n",
      "        EOG-eeg--0.200-0.200-PCA-01 (1 x 24) active\n",
      "        EOG-eeg--0.200-0.200-PCA-02 (1 x 24) active\n",
      "        Average EEG reference (1 x 24) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    1000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "Not setting metadata\n",
      "280 matching events found\n",
      "No baseline correction applied\n",
      "Created an SSP operator (subspace dimension = 3)\n",
      "3 projection items activated\n"
     ]
    }
   ],
   "source": [
    "epoch_file = '../preproc/BCI3_ssp_car_ica_[3]_P3_epo.fif'\n",
    "epochs = mne.read_epochs(epoch_file)\n",
    "event_ids = epochs.event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projections have already been applied. Setting proj attribute to True.\n",
      "Applying baseline correction (mode: mean)\n",
      "Dropped 202 epochs: 0, 1, 2, 3, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279\n"
     ]
    }
   ],
   "source": [
    "epochs = epochs.apply_proj().copy()\n",
    "epochs = epochs.apply_baseline().copy()\n",
    "epochs.equalize_event_counts()\n",
    "tmin, tmax = -0.5, 2\n",
    "# epochs.crop(tmin= tmin, tmax=tmax)\n",
    "picks=['C1', 'C2','C3', 'C4', 'C5', 'C6', 'Cz']#,'T9','T10']\n",
    "epochs.pick_channels(picks)\n",
    "T1 = epochs['right']\n",
    "T2 = epochs['foot']\n",
    "t1 = epochs['right'].average()\n",
    "t2 = epochs['foot'].average()\n",
    "classes = list(event_ids.keys())\n",
    "# mne.viz.plot_compare_evokeds([T1,-T2],picks= c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = epochs.events[:,-1].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7937,     0,     1],\n",
       "       [ 8501,     0,     1],\n",
       "       [ 9045,     0,     1],\n",
       "       [ 9598,     0,     1],\n",
       "       [10148,     0,     1],\n",
       "       [12970,     0,     1],\n",
       "       [13553,     0,     1],\n",
       "       [14090,     0,     1],\n",
       "       [14625,     0,     1],\n",
       "       [15196,     0,     1],\n",
       "       [15769,     0,     1],\n",
       "       [17436,     0,     1],\n",
       "       [18552,     0,     1],\n",
       "       [21198,     0,     1],\n",
       "       [21769,     0,     1],\n",
       "       [22866,     0,     1],\n",
       "       [23997,     0,     1],\n",
       "       [25103,     0,     1],\n",
       "       [25671,     0,     1],\n",
       "       [26217,     0,     1],\n",
       "       [26787,     0,     1],\n",
       "       [27892,     0,     1],\n",
       "       [28448,     0,     1],\n",
       "       [30102,     0,     1],\n",
       "       [30683,     0,     1],\n",
       "       [31248,     0,     1],\n",
       "       [31785,     0,     3],\n",
       "       [32922,     0,     3],\n",
       "       [33495,     0,     3],\n",
       "       [34070,     0,     3],\n",
       "       [37297,     0,     3],\n",
       "       [37853,     0,     3],\n",
       "       [38417,     0,     3],\n",
       "       [38955,     0,     3],\n",
       "       [40061,     0,     3],\n",
       "       [40607,     0,     3],\n",
       "       [41157,     0,     3],\n",
       "       [42287,     0,     3],\n",
       "       [42847,     0,     3],\n",
       "       [43427,     0,     3],\n",
       "       [44545,     0,     3],\n",
       "       [46216,     0,     3],\n",
       "       [47340,     0,     3],\n",
       "       [48455,     0,     3],\n",
       "       [48996,     0,     3],\n",
       "       [50678,     0,     3],\n",
       "       [53344,     0,     3],\n",
       "       [53885,     0,     3],\n",
       "       [54972,     0,     3],\n",
       "       [56630,     0,     3],\n",
       "       [57769,     0,     3],\n",
       "       [58321,     0,     3],\n",
       "       [58880,     0,     2],\n",
       "       [60036,     0,     2],\n",
       "       [60591,     0,     2],\n",
       "       [61173,     0,     2],\n",
       "       [62279,     0,     2],\n",
       "       [62838,     0,     2],\n",
       "       [63949,     0,     2],\n",
       "       [64508,     0,     2],\n",
       "       [65083,     0,     2],\n",
       "       [65626,     0,     2],\n",
       "       [66179,     0,     2],\n",
       "       [67266,     0,     2],\n",
       "       [67841,     0,     2],\n",
       "       [69524,     0,     2],\n",
       "       [76111,     0,     2],\n",
       "       [76649,     0,     2],\n",
       "       [77232,     0,     2],\n",
       "       [78366,     0,     2],\n",
       "       [78945,     0,     2],\n",
       "       [79480,     0,     2],\n",
       "       [80030,     0,     2],\n",
       "       [80613,     0,     2],\n",
       "       [81716,     0,     2],\n",
       "       [82275,     0,     2],\n",
       "       [82845,     0,     2],\n",
       "       [83968,     0,     2]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 25, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 24, 11, 12,  2,\n",
       "        3,  4,  5,  1,  7,  8,  9, 10,  6, 54, 63, 55, 56, 57, 58, 59, 60,\n",
       "       61, 62, 64, 70, 66, 67, 68, 69, 71, 72, 73, 74, 75, 53, 65, 52, 77,\n",
       "       76, 51, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41,\n",
       "       42, 43, 44, 45, 46, 47, 48, 49, 50, 38])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.events[ff,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs.plot_image();\n",
    "# epochs.plot_psd();\n",
    "# scale = dict(mag=1e-12, grad=4e-11, eeg=100e-6)\n",
    "# epochs.plot(scalings = scale);\n",
    "\n",
    "# t1.plot_joint(times = (0.19,2.47));\n",
    "\n",
    "# t2.plot_joint(times = (0.19,2.47));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: (78, 7, 151)\n",
      "Train Size: (78, 7, 151)\n"
     ]
    }
   ],
   "source": [
    "epochs_data = epochs.get_data()\n",
    "# labels = epochs.events[epochs.events[:,-1]!=3]\n",
    "labels = epochs.events[:,-1]\n",
    "labels = labels-2\n",
    "# labels = epochs.events[:,-1]-2\n",
    "print(f\"Data Size: {epochs_data.shape}\")\n",
    "\n",
    "# train data \n",
    "tmin, tmax = -0.5, 1\n",
    "epochs_train = epochs.copy().crop(tmin= tmin, tmax=tmax)\n",
    "epochs_train_data = epochs_train.get_data()\n",
    "# epochs_train_data = np.transpose(epochs_train_data, axes = (1, 2, 0)) # Sklearn data = n_sam * n_freq\n",
    "print(f\"Train Size: {epochs_train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot == True:\n",
    "    clas = 'right'\n",
    "    # epochs['right'].plot_image(combine = 'mean', title = 'right');\n",
    "    epochs[clas].average().plot_image(show_names = 'all', titles = clas, picks = picks);\n",
    "    epochs[clas].plot_topo_image(title = clas);\n",
    "    epochs[clas].plot_image(title = clas, combine = 'mean');\n",
    "    epochs[clas].plot_psd();\n",
    "    epochs[clas].average().plot_joint(picks=picks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import tfr_morlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies =  np.arange(5,30,1) #np.logspace(*np.log10([5, 30]), num=25)\n",
    "chpicks = [1] #baseline = (-0.5, 0.),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = tfr_morlet(epochs[classes[0]], freqs = frequencies, n_cycles = frequencies/2, return_itc= False)\n",
    "if plot == True:\n",
    "    for pick in chpicks:\n",
    "        # power.plot(picks=[pick],  vmin=-0.00003, vmax=0.00003, title=f'{pick}')#, mode='logratio')\n",
    "        # power.plot_joint(timefreqs = [(0.19,10),(2.47,10)],mode = 'logratio', title = classes[0])\n",
    "        power.plot_joint(mode = 'mean', title = classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = tfr_morlet(epochs[classes[1]], freqs = frequencies, n_cycles = frequencies/2, return_itc= False)\n",
    "if plot == True:\n",
    "    for pick in chpicks:\n",
    "        # power.plot(picks=[pick],  vmin=-0.00003, vmax=0.00003, title=f'{pick}')#, mode='logratio')\n",
    "        power.plot_joint(timefreqs = [(0.19,10),(2.47,10)],mode = 'logratio', title = classes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/f removal- Power Normalization - Average TFR\n",
    "clas = 'foot'\n",
    "baset = [epochs.baseline[1] , -0.2]\n",
    "power = tfr_morlet(epochs[clas], freqs = frequencies, n_cycles = frequencies/2, return_itc= False)\n",
    "powerDB = np.zeros(power.data.shape)\n",
    "t = np.arange(np.abs(baset[1]-baset[0])*epochs.info['sfreq'], dtype = np.int64)\n",
    "for ch in range(power.data.shape[0]):\n",
    "    for f in range(power.data.shape[1]):\n",
    "        baseline = np.mean(power.data[ch,f,t])\n",
    "        activity = power.data[ch,f,:]\n",
    "        powerDB[ch,f,:] = 10*np.log10(activity/baseline)\n",
    "\n",
    "newPower = mne.time_frequency.AverageTFR(epochs.info, powerDB,epochs.times, frequencies,1).copy()\n",
    "if plot == True:\n",
    "    newPower.plot_joint(title = clas, mode = 'mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/mangaldeep/HDD2/workspace/MotionControl_MasterThesis/main/feature_extraction/tf.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/mangaldeep/HDD2/workspace/MotionControl_MasterThesis/main/feature_extraction/tf.ipynb#ch0000020?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m((epochs\u001b[39m.\u001b[39mevents[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39munique())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "len((epochs.events[:,-1]).unique())\n",
    "# ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/f removal- Power Normalization - Epochs TFR\n",
    "def powerEPdBcalc(clas):\n",
    "    baset = [epochs.baseline[1] , -0.2]\n",
    "    power = tfr_morlet(epochs[clas], freqs = frequencies, n_cycles = frequencies/2, return_itc= False, average=False)\n",
    "    shp = power.data.shape\n",
    "    powerdB = np.zeros((shp[0],shp[2],shp[3]))\n",
    "    powerEp = powerdB.copy()\n",
    "    t = np.arange(np.abs(baset[1]-baset[0])*epochs.info['sfreq'], dtype = np.int64)\n",
    "    for ep in range(power.data.shape[0]):\n",
    "        EPpower  = np.mean(power.data[ep,:,:,:],axis=1)\n",
    "        for f in range(EPpower.shape[0]):\n",
    "            baseline = np.mean(EPpower[f,t])\n",
    "            activity = EPpower[f,:]\n",
    "            powerdB[ep,f,:] = 10*np.log10(activity/baseline)\n",
    "            powerEp[ep,f,:] = activity\n",
    "    return powerdB, powerEp\n",
    "\n",
    "# newEPPower = tfr_morlet(epochs[clas], freqs = frequencies, n_cycles = frequencies/2, return_itc= False, average=False)\n",
    "powerEpdB, powerEp = {}, {}\n",
    "powerEpdB[classes[0]], powerEp[classes[0]]= powerEPdBcalc(classes[0])\n",
    "powerEpdB[classes[1]], powerEp[classes[1]]= powerEPdBcalc(classes[1])\n",
    "\n",
    "# newPower.plot_joint(title = clas, mode = 'mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerEpdB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.vstack((powerEp[classes[0]],powerEp[classes[1]]))\n",
    "# train_data = train_data.reshape(train_data.shape[0],-1)\n",
    "labels = np.concatenate([-np.ones(powerEpdB[classes[0]].shape[0]),\n",
    "                     np.ones(powerEpdB[classes[1]].shape[0])])\n",
    "print(train_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot == True:\n",
    "    scaler = MinMaxScaler((0,1))\n",
    "    data = scaler.fit_transform(powerEp[classes[0]][10,:,:])\n",
    "    plt.figure()\n",
    "    plt.imshow(data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LDA()\n",
    "cv = ShuffleSplit(10, test_size = 0.2, random_state=1)\n",
    "cv_split = cv.split(train_data, labels)\n",
    "scaler = MinMaxScaler((0,1))\n",
    "train_data = scaler.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('scatter', scat_obj), ('clf', clf)])\n",
    "# scores = cross_val_score(pipe, epochs_train_data, labels, cv = cv, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = epochs.info['sfreq']\n",
    "w_length = int(sfreq * 0.05)   # running classifier: window length\n",
    "w_step = int(sfreq * 0.01)  # running classifier: window step size\n",
    "w_start = np.arange(0, train_data.shape[1] - w_length, w_step)\n",
    "\n",
    "scores_windows = []\n",
    "\n",
    "for train_idx, test_idx in cv_split:\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "    X_train = train_data[train_idx]\n",
    "    X_test = train_data[test_idx]\n",
    "\n",
    "    # fit classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # running classifier: test classifier on sliding window\n",
    "    score_this_window = []\n",
    "    # for n in w_start:\n",
    "    #     X_test = (train_data[test_idx]) #[:, n:(n + w_length)])\n",
    "    scores_windows.append(clf.score(X_test, y_test))\n",
    "    # scores_windows.append(score_this_window)\n",
    "w_times = (w_start + w_length / 2.) / sfreq + epochs.tmin\n",
    "np.mean(scores_windows)\n",
    "# plt.scatter(scores_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True, progress=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "alexnet.eval();\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freesze model parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# change final layer of alexnet model\n",
    "alexnet.classifier[6] = nn.Linear(4096,2)\n",
    "alexnet.classifier.add_module('7',nn.LogSoftmax(dim=1))\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(alexnet,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "scaler = MinMaxScaler((0,1))\n",
    "data = scaler.fit_transform(powerEp[classes[0]][10,:,:])\n",
    "input_image = Image.fromarray(np.uint8(cm.gist_earth(data)*255)[:,:,:3])\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(227),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(powerEpdB[classes[0]][0])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(powerEp[classes[0]][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.uint8(cm.gist_earth(data)*255).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(im,  interpolation='nearest', cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
