\section*{Introduction}
The data after preprocessing is clean from noise and other artifcats, now the most informative and distinguishing features are extracted for classification. This chapter discusses about the conventional signal processing pipeline that is widely researched in the BCI community. These methods are proven to be robust and several existingBCI systems are implented with these algorithms.

\section{Feature Extraction}
Brain signals are transient as well as diffuse spike and action potential. It is rhythemic and hence could be sinusoidal or non-sinusoidal and includes small oscillaions of semi periodic activations. Therefore the local information exists in time or frequency domain or both. \par

The brain signals could be time-locked i.e. the activity in the brain is time synchronized with the stimulus or the temporal dynamics is the same across every trials at the same instance. It could be phase-locked as well i.e. the phase angle time series is the same or similar on every trial. Depending on the type of the experiment used to gather data and the EEG paradigm, these paramters influence the choice of feature extraction mechanism. 

\subsection{Time Domain Analysis}
Time domain analysis is useful in extracting temporal features in the preprocessed measurement. Some of the commonly used time domain analysis methods include signal averaging, Hjorth parameters, fractal dimensions, auto regressive models, Bayesian filtering, Kalman filtering, particle filtering, zero crossing, template matching, power and window detection.\par

Time domain analysis leads to loss of information on averaging several trials in case of non-phase locked brain signals, but frequency analysis doesn't lose information on averaging trials.

\subsection{Frequency Domain Analysis}
Transforming the preprocessed data into frequency domain provides more useful information about the phase and amplitude of the brain signal required for analysis. Imagined movements leads to oscillations in premotor and sensorimotor areas which is observed as amplitude changes in the $\mu$, $\beta$ and $\gamma$ region. These features can be used to create feature vectors. Some of the common frequency domain analysis are fourier transform, short-time fourier transform and welch transform. These methods provide information on the spectral contents of the signal but not the temporal information of the event. 

\subsubsection{Fourier Transform (FT)}
Fourier transform decomposes signal into a weighted sum of sine and cosine waves of different frequencies. It works on the assumptions that the signal is infinite, periodic and stationary. Brain signals do not hold to these assumptions. These shortcomings can be over come by using short window periods for analysis, termed as Short Time Frequency Transform (STFT). Another commonly used method for frequency domain analysis is welch transform that provides smooth spectral decomposition. \par

Phase content is not used in brain signal analysis in most cases as the signal could be non-phase locked. Hence amplitude information is mostly used in feature extraction. Power spectrum \ref{eq:pwr} obtained by square of amplitude in different frequency components during course of the task is the feature of interest in most BCI experiments.

\begin{equation} \label{eq:pwr}
    P(n) = A(n)^2
\end{equation}
where $A$ is the amplitude spectrum, $P$ is the Power Spectrum and $n$ denotes the index of the content.

\subsection{Time-Frequency Analysis (TFA)}
The method discussed above do not provide temporal information on occurence of the event. Time-Frequency analysis on the signal provides temporal-spectral information. One of the most commonly used TFA method is wavelet transform

\subsubsection{Wavelet Transform (WT)}
Wavelet transform uses finite basis functions called wavelets that are scaled and translated copies of finite length waveform called mother wavelet. Wavelets divide the signals of interest into different frequency components, each component can be studied at a resolution matched for its scale. A large scale component produces coarse resolution and small scale component produces fine resolution. The wavelet transform provides lower frequency resolution and higher temporal resolution at higher frequencies whereas higher frequency resolution and lower temporal resolution at lower frequencies. \par

Wavelet analysis is performed by convolution of wavelets of different scale and resolution with the signal of interest. The result of convolution contains components of the signal that are in the same frequency of the wavelet used. Scalogram provides graphical way to interpret the time and frequency information of the signal. The transform could be either discrete or continuous depending upon the scale and translation parameters. Contininous Wavelet Transform (CWT) provides more resolution than the Discrete Wavelet Transform (DWT).

A generic equation for a wavelet $\psi$ used in CWT can be written as \ref{eq:cwt}.
\begin{equation} \label{eq:cwt}
    \psi_{a,b}(t) = \frac{1}{\sqrt{a}} \psi( \frac{t - b}{a}) 
\end{equation}
where $t$ denotes time, $b$ is the shift parameter that slides the waveform along the time axis and $a$ is the scale factor that is used to scale the frequency of the wavelet.

Generally for a given wavelet function $\psi(\frac{t}{a})$ when $a > 1$ the mother wavelet dilates and it helps in capturing low frequency component and when $0 < a < 1$ it compresses the mother wavelet, capturing the high frequency componenets in the signal of interest. Thus $a \propto \frac{1}{f}$.

Wavelets have bandpass characteristics, with sum of all points being zero \ref{eq:wt_sum0} and its equivaent frequency can be determined by \ref{eq:wt_cf}.

\begin{equation} \label{eq:wt_sum0}
    \int\psi(t) = 0
\end{equation}

\begin{equation} \label{eq:wt_cf}
    F_{eq} = \frac{C_f}{a\delta_t}
\end{equation}
where $F_{eq}$ is the equivalent frequency, $C_f$ is the centre frequency, $a$ is the scaling factor and $\delta_t$ is the sampling intreval.

A generic equation for a wavelet used in DWT can be written as \ref{eq:dwt}.
\begin{equation} \label{eq:dwt}
    \psi_{a,b}(t) = \frac{1}{2^{\frac{j}{v}}} \psi( \frac{n - m}{2^{\frac{j}{v}}}) 
\end{equation}
where $j$ is the scaling parameter, $v(>1)$ denotes voices per octave (lograthamic unit of ratio between two frequencies) and $m$ is the translational parameter.

% Table about wavelet scale

Various types of wavelets are used in research and one of the commonly used wavelet is the Morlet Wavelet. It the product of complex valued sine wave and gaussian. In frequency domain, The amplitude spectrum of the complex morlet wavelet is not symmetrical and the power spectrum is independent of the phase relation between the wavelet and signal. The Complex Morlet Wavelet can be written as \ref{eq:Mrlt1}.

\begin{equation} \label{eq:Mrlt1}
    \psi_\lambda(t) = e^{i2\pi ft -0.5(\frac{t}{\sigma})^2}
\end{equation}
where $\sigma = \frac{n}{2\pi f}$, $n$ is the number of cycles, $\lambda$ denotes a wavelet with specific scale and resolution parameters, $f$ is the frequency of the complex sine wave used.

The equation \ref*{eq:Mrlt} can also be written as  \ref*{eq:Mrlt2}

\begin{equation} \label{eq:Mrlt2}
    \psi_\lambda(t) = e^{i2\pi ft} e^{-4\ln(2)\frac{t^2}{h^2}}
\end{equation}

A wavelet transform can be written as \ref{eq:WT_wx}.

\begin{equation} \label{eq:WT_wx}
    \mathcal{W}x  = \int x \ast \psi_\lambda(t) dt
\end{equation}

where $h$ is the full width half maximum (FWHM) parameter. With higher $n$, better temporal resolution and with lower $n$ better spectral resolution is achieved. Most algorithms work by starting with low number of cycles and gradually increasing to higher number of cycles. However the results of the wavelet convolution are easily interpretable only for stationary signals within the FWHM of the wavelet. With the right design of the wavelet this can be achieved.

\subsection{Wavelet Scaterring Transform (WST)}
The wavelets are translated and convoluted with the signal of interest, as the wavelets commutes with translations the resulting expression becomes translation covariant. i.e. Shifting the signal also shifts wavelet coefficients, therefore the comparison between translated signals become difficult. In order to make proper representation of the signal the transform has to be translation invariant, stable under deformation and offer good structural information of all frequencies.

By Wavelet Scaterring Transform (WST) non-informative variability in the signal such as translation, rotation and scaling are discarded. It can be suited for any application, with more data a pre-trained model is used as a pipeline and for small datasets wavelet scattering initalization could be performed. WST yields representations that are translation invariant and stable against time warping deformations. This is achieved by subsequent non-linearity and averaging steps after convolving with the wavelet. 

CWT is the basic building block for WST. CWT measures similarity of the signal with wavelets of varying frequency and scale at each point in time. For a given wavelet $\psi$, it is first shifted by time $\tau$, the similarity if found by computing the inner product $\left\langle x, \psi \right\rangle$. This is repeated for all $\tau$ and $\psi$.

Any linear operation which is translation invariant of a wavelet coefficient will result in zero, which is not informative. Hence a non-linear invariant operand is required. Modulus being a non-linear, optimal contractive operator, when applied on CWT as given in \ref{eq:wst_TI}, removes the phase information in the signal and makes it invariant to translations,  that gives a measure on sparsity of the wavelet coefficients. The coefficients obtained are called as unaveraged coefficients. 

\begin{equation} \label{eq:wst_TI}
    \begin{split}
     \mathcal{U} x = |\mathcal{W} x|\\
     \int | x \ast \psi_\lambda(t) | dt = \| x \ast \psi_{\lambda_{1}}\|_1 
    \end{split}
\end{equation}
where $\lambda_1 = 2^j$ is the centre frequency of the first order wavelets, with $j$ scale factor.

The unaveraged coefficients when convolved again with a lowpass or scaling filter(also called father wavelet) $\phi$ as given in \ref{eq:wst_def} performs temporal averaging of the signal. The coefficients obtained are called Scattering coefficients. It imposes stability against time warping deformations. This however removes high frequency information, but can be recovered by performing WST again on the unaveraged coefficients.

\begin{equation} \label{eq:wst_def}
    \begin{split}
        \mathcal{S} x = \mathcal{U} x \ast \phi\\
        \mathcal{S} x(t, \lambda_1) = \| x \ast \psi_{\lambda_{1}}\|_1 \ast \phi         
    \end{split}
\end{equation}

The above steps are performed recursively (usually 3 orders) on the results on the unaveraged coefficients and the features are extracted at each level by convolving the results with lowpass filter. Scatter representations consists of {0,1,2} order coefficients which are generated by composing wavelets in different sequences. Multiple wavelets composed together called filter banks capture high frequency components. Filter bank consists of several dilated and rotated wavelets with no orthogonality.

Consider family of wavelets ${\psi_{\lambda}}$

The dilated wavelet can be written as \ref{eq:wst_dila}

\begin{equation} \label{eq:wst_dila}
    \psi_{\lambda}(t) = 2^{\frac{-j}{Q}}\psi(2^{\frac{-j}{Q}}t)
\end{equation}
where $Q$ is number of bandpass filter in an octave.

\begin{equation} \label{eq:wst_wt}
    \mathcal{W} x = \{x\ast \phi(t) , x\ast \psi_\lambda(t) \}
\end{equation}

The second order coefficients obtained can be decorrelated to increase their invariance through renormalization. Different features are obtained at each order of coefficients. The first order coefficients represents the frequency content (oscillation rate) in the signal, the second order coefficients represents rates of frequency change, treat every rate from first order as its own signal and repeat first order upon each modulus. 

 \subsubsection{Exponential decay of scattering coefficients}
 Repeated application of modulo moves the energy contained in the high frequencies of the initial signal towards low frequencies. Convolution with the lowpass filter picks that energy and used in  feature extraction. At each step of the scattering transform, energy in the lowest frequency bands is output by convolution with lowpass filter at its respective step. The remaining part of the energy  is shifted towards the lower frequencies by applying modulus of wavelet transform.
 
 \begin{equation} \label{eq:wst_wt}
    \mathcal{W} x = \{x\ast \phi(t) , x\ast \psi_\lambda(t) \} \\
\end{equation}

If $|\hat{\phi}(w)|^2 + \sum_{\lambda} | \hat{\psi}_{\lambda}(w)|^2 = 1$ then $\mathcal{W}$ is unitary.

\begin{equation}    
    \|\mathcal{W}x\|^2 = \|x\ast\phi\|^2 + \sum_{\lambda}\| x \ast \psi_\lambda\| = \|x\|^2
\end{equation}

 \subsubsection{Neural Network representation of WST}
 Typical Convolution Neural Network (CNN) involves, succesive operations involving convolution, non-linearity, pooling/subsampling. WST performs the same operations, hence WST can be represented as a neural network (WSN) to extract the scattering coefficients. This can be highly efficient as the wavelets values for the filterbank is predetermined hence learning is not required and no complex architecture has to be designed. Kymatio is an Open-source WST implentation that leaverages machine learning framework like PyTorch, Keras or Scikit-learn in the backend.

\subsection{Spatial Analysis}
The spatial filters invert the measurement to its original source. Spatial Analysis helps in mapping the source signals to the brain topography that can be used for further topographical studies. In general Patterns are not Filters. Patterns denote the propogation of a source to sensors which is used in forward model \ref*{eq:fwd_model}. Filters on the other hand denote weighting of EEG channels and it is used in backwward model \ref*{eq:bck_model} to extract individual components.

\begin{equation} \label{eq:fwd_model}
    x(t) = A s(t) +n(t)
\end{equation}
where $s$ can be a vector denoting sources. $n$ is a noise vector and $x$ is the measurement at the channel. A is the forward or mixing matrix in which every column corresponds to a spatial pattern.

\begin{equation} \label{eq:bck_model}
    \hat{s}(t) = W^T x(t)
\end{equation}
where $W$ represnts the unmixing matrix where every row of $W^T$ corresponds to spatial filter.

The spatial map of a filter is hard to interpret, but patterns are easy to interpret and hence patterns corresponding to a given spatial filter is found.

\subsubsection{Common Spatial Patterns (CSP)}
CSP is a supervised feature extraction method for classification algorithms which works only for motor-imagery information in EEG. It learns to optimally discriminate band power features. Variance of the band pass signal is proportional to the band power of the frequency band. CSP leverages this by finding patterns where the  variance of filtered data from one class is maximized while variance of filtered data from other class is minimized. The resulting feature vector enhance discriminability between different classes. However it works on the assumptions that frequency band and time window of the measurements are known, source activity constellation differs between two classes and the band passed signal is jointly gaussian within the time window. 

Consider measurement data set $\{\mathbb{X}^{i}_{c}\}^{k}_{i=1}$ for trial $i$ belonging to class $c \in\{1,2\}$. Each $\{\mathbb{X}^{i}_{c}\}$ is a $N * T$ matrix, where N is the number of channels and T is the number of samples per channel. The goal of CSP is to find $W$ given by $N * M$, consisting M spatial filters i.e. each column is a  spatial filter, that transforms the measured signals \ref{eq:trans_csp}.

\begin{equation} \label{eq:trans_csp}
    x_{CSP}(t) = W^{T}x(t)
\end{equation}
where $x(t)$ is vector of input signals at time $t$ from all channels. CSP can formally be defined for a two class problem as \ref{eq:def_csp}.

\begin{equation} \label{eq:def_csp}
    \begin{split}
        J(w) = \frac{WX_{1}X_{1}^TW^T}{WX_{2}X_{2}^TW^T} \\
         = \frac{WC_{1}W^T}{WC_{2}W^T}
    \end{split}
\end{equation}
where $C_{c}$ is the spatial covariance matrix for class $c$, $W$ is the spatial filter to optimize and $X_{c}$ is the multichannel EEG signal from class $c$. The problem could be solved to either minimize $J(w)$ where variance in $X_{1}$ is minimized and maximized in $X_{2}$ or maximize $J(w)$ where variance in $X_{1}$ is maximized and minimized in $X_{2}$.

This can be computed by Generaized Eigen Value Decomposition (GEVD) of $C_{1}$ and $C_{2}$. The Eigen vectors corresponding to the largest eigen value will maximize $J(w)$. and eigen vectors corresponding to the smallest eigen value will minimize $J(w)$. 

\begin{equation} \label{eq:def_csp}
    \begin{split}
        WC_{1}W^T = \Lambda_{1}\\
        WC_{2}W^T = \Lambda_{2}
    \end{split}
\end{equation}
where  $\Lambda_{1}$ and  $\Lambda_{2}$ are diagonal matrices containing eigen values such that $\Lambda_{1} + \Lambda_{2} = \mathbf{I}$

\begin{equation} \label{eq:def_gevd}
    WC_{1}W^T = \Lambda W^T(C_{1} + C_{2})W = \mathbf{I}
\end{equation}

Typically 3 CSP filter pairs i.e. minimization and maximization are used which results in 6 feature vectors that forms the filter matrix $W$. Once filter is obtained, the prediction function using CSP can be defined as \ref{eq:feat_csp}.

\begin{equation} \label{eq:feat_csp}
    \begin{split}
    y = sign(\theta log(var(WX)) +b) \\
        = sign(\theta log(WCW^T) + b)
    \end{split}
\end{equation}
The application of log on the spatially filtered data makes it a gaussian distribution. The classifier applied can be either linear or nonlinear. After applying CSP the data is rotated and placed orthogonal.

CSP is simple, computationally efficient with high classification performance. But it is not robust to noise and non-stationaries, prone to overfitting and requires many training examples. Several variants of CSP are researched and developed such as Filter Bank CSP, Regularized CSP, Invariant CSP\dots to overcome the disadvantages of vannila CSP.

% \section{Feature Selection} 

\section{Classification}
With the required features extracted, the vectors can be forwarded to the classifier as a las t step in the BCI pipeline. Some of the most commonly used classifiers include SVM, LDA\dots These could also be neural networks, but neural network could also perform feature extraction, hence it would not be efficient to use neural network just for classification.

\subsection{Support Vector Machine (SVM)}

\subsection{Linear Discriminant Analysis (LDA)}

\section*{Summary}