{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk \n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/media/mangaldeep/HDD2/workspace/MotionControl_MasterThesis')\n",
    "from main.extraction.physionet_MI import extractPhysionet\n",
    "from main.extraction.bci3_IVa import extractBCI3\n",
    "from data import brain_atlas  as bm\n",
    "plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physionet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = extractPhysionet(runs = [3], person_id = 3)\n",
    "# # raw.pick_channels(['C3','C4'])\n",
    "# raw.notch_filter(60)\n",
    "# raw = raw.filter(7,50,verbose = False).copy()\n",
    "\n",
    "# event_data = mne.events_from_annotations(raw)\n",
    "# event_marker, event_ids = event_data\n",
    "# event_ids = dict({'T1':2, 'T2':3})\n",
    "# epochs = mne.Epochs(raw, event_marker, event_ids, tmin=-2, tmax=4, preload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_file = '../preproc/Physionet_trial_epo.fif'\n",
    "# epochs = mne.read_epochs(epoch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = epochs.apply_proj().copy()\n",
    "# epochs = epochs.apply_baseline((-2.0,-0.2)).copy()\n",
    "# epochs.equalize_event_counts()\n",
    "# tmin, tmax = -0.5, 2\n",
    "# epochs.crop(tmin= tmin, tmax=tmax)\n",
    "# picks=['C1', 'C2','C3', 'C4', 'C5', 'C6', 'Cz']#,'T9','T10']\n",
    "# epochs.pick_channels(picks)\n",
    "# T1 = epochs['T1']\n",
    "# T2 = epochs['T2']\n",
    "# t1 = epochs['T1'].average()\n",
    "# t2 = epochs['T2'].average()\n",
    "# # mne.viz.plot_compare_evokeds([T1,-T2],picks= c);\n",
    "# classes = list(event_ids.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCI3 -IVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = extractBCI3(runs = 0, person_id = 3)\n",
    "# raw = raw.filter(7,30,verbose = False).copy()\n",
    "\n",
    "# event_data = mne.events_from_annotations(raw)\n",
    "# event_marker, event_ids = event_data\n",
    "# event_ids = dict(right = 1, foot = 3, test = 2)\n",
    "# epochs = mne.Epochs(raw, event_marker, event_ids, tmin=-0.5, tmax=1, preload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../preproc/BCI3_car_[3]_P3_epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 24) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    1000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "Not setting metadata\n",
      "280 matching events found\n",
      "No baseline correction applied\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n"
     ]
    }
   ],
   "source": [
    "epoch_file = '../preproc/BCI3_car_[3]_P3_epo.fif'\n",
    "epochs = mne.read_epochs(epoch_file)\n",
    "event_ids = epochs.event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projections have already been applied. Setting proj attribute to True.\n",
      "Applying baseline correction (mode: mean)\n",
      "Dropped 202 epochs: 0, 1, 2, 3, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279\n"
     ]
    }
   ],
   "source": [
    "epochs = epochs.apply_proj().copy()\n",
    "epochs = epochs.apply_baseline().copy()\n",
    "epochs.equalize_event_counts()\n",
    "tmin, tmax = -0.5, 2\n",
    "# epochs.crop(tmin= tmin, tmax=tmax)\n",
    "picks=['C1', 'C2','C3', 'C4', 'C5', 'C6', 'Cz']#,'T9','T10']\n",
    "epochs.pick_channels(picks)\n",
    "T1 = epochs['right']\n",
    "T2 = epochs['foot']\n",
    "t1 = epochs['right'].average()\n",
    "t2 = epochs['foot'].average()\n",
    "classes = list(event_ids.keys())\n",
    "# mne.viz.plot_compare_evokeds([T1,-T2],picks= c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table class=\"table table-hover\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        <td>foot: 26<br>right: 26<br>test: 26<br></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>-0.500 – 1.000 sec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>-0.500 – 0.000 sec</td>\n",
       "    </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<EpochsFIF |  78 events (all good), -0.5 - 1 sec, baseline -0.5 – 0 sec, ~660 kB, data loaded,\n",
       " 'foot': 26\n",
       " 'right': 26\n",
       " 'test': 26>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs.plot_image();\n",
    "# epochs.plot_psd();\n",
    "# scale = dict(mag=1e-12, grad=4e-11, eeg=100e-6)\n",
    "# epochs.plot(scalings = scale);\n",
    "\n",
    "# t1.plot_joint(times = (0.19,2.47));\n",
    "\n",
    "# t2.plot_joint(times = (0.19,2.47));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: (78, 7, 151)\n",
      "Train Size: (78, 7, 151)\n"
     ]
    }
   ],
   "source": [
    "epochs_data = epochs.get_data()\n",
    "# labels = epochs.events[epochs.events[:,-1]!=3]\n",
    "labels = epochs.events[:,-1]\n",
    "labels = labels-2\n",
    "# labels = epochs.events[:,-1]-2\n",
    "print(f\"Data Size: {epochs_data.shape}\")\n",
    "\n",
    "# train data \n",
    "tmin, tmax = -0.5, 1\n",
    "epochs_train = epochs.copy().crop(tmin= tmin, tmax=tmax)\n",
    "epochs_train_data = epochs_train.get_data()\n",
    "# epochs_train_data = np.transpose(epochs_train_data, axes = (1, 2, 0)) # Sklearn data = n_sam * n_freq\n",
    "print(f\"Train Size: {epochs_train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot == True:\n",
    "    clas = 'foot'\n",
    "    # epochs['right'].plot_image(combine = 'mean', title = 'right');\n",
    "    epochs[clas].average().plot_image(show_names = 'all', titles = clas, picks = picks);\n",
    "    epochs[clas].plot_topo_image(title = clas);\n",
    "    epochs[clas].plot_image(title = clas, combine = 'mean');\n",
    "    epochs[clas].plot_psd();\n",
    "    epochs[clas].average().plot_joint(picks=picks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import tfr_morlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies =  np.arange(5,30,1) #np.logspace(*np.log10([5, 30]), num=25)\n",
    "chpicks = [1] #baseline = (-0.5, 0.),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = tfr_morlet(epochs[classes[0]], freqs = frequencies, n_cycles = frequencies/2, return_itc= False)\n",
    "if plot == True:\n",
    "    for pick in chpicks:\n",
    "        # power.plot(picks=[pick],  vmin=-0.00003, vmax=0.00003, title=f'{pick}')#, mode='logratio')\n",
    "        # power.plot_joint(timefreqs = [(0.19,10),(2.47,10)],mode = 'logratio', title = classes[0])\n",
    "        power.plot_joint(mode = 'mean', title = classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = tfr_morlet(epochs[classes[1]], freqs = frequencies, n_cycles = frequencies/2, return_itc= False)\n",
    "if plot == True:\n",
    "    for pick in chpicks:\n",
    "        # power.plot(picks=[pick],  vmin=-0.00003, vmax=0.00003, title=f'{pick}')#, mode='logratio')\n",
    "        power.plot_joint(timefreqs = [(0.19,10),(2.47,10)],mode = 'logratio', title = classes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/f removal- Power Normalization - Average TFR\n",
    "clas = 'right'\n",
    "baset = [epochs.baseline[1] , -0.2]\n",
    "power = tfr_morlet(epochs[clas], freqs = frequencies, n_cycles = frequencies/2, return_itc= False)\n",
    "powerDB = np.zeros(power.data.shape)\n",
    "t = np.arange(np.abs(baset[1]-baset[0])*epochs.info['sfreq'], dtype = np.int64)\n",
    "for ch in range(power.data.shape[0]):\n",
    "    for f in range(power.data.shape[1]):\n",
    "        baseline = np.mean(power.data[ch,f,t])\n",
    "        activity = power.data[ch,f,:]\n",
    "        powerDB[ch,f,:] = 10*np.log10(activity/baseline)\n",
    "\n",
    "newPower = mne.time_frequency.AverageTFR(epochs.info, powerDB,epochs.times, frequencies,1).copy()\n",
    "if plot == True:\n",
    "    newPower.plot_joint(title = clas, mode = 'mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 25, 151)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPower.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "Not setting metadata\n"
     ]
    }
   ],
   "source": [
    "# 1/f removal- Power Normalization - Epochs TFR\n",
    "def powerEPdBcalc(clas):\n",
    "    baset = [epochs.baseline[1] , -0.2]\n",
    "    power = tfr_morlet(epochs[clas], freqs = frequencies, n_cycles = frequencies/2, return_itc= False, average=False)\n",
    "    shp = power.data.shape\n",
    "    powerdB = np.zeros((shp[0],shp[2],shp[3]))\n",
    "    t = np.arange(np.abs(baset[1]-baset[0])*epochs.info['sfreq'], dtype = np.int64)\n",
    "    for ep in range(power.data.shape[0]):\n",
    "        EPpower  = np.mean(power.data[ep,:,:,:],axis=1)\n",
    "        for f in range(EPpower.shape[0]):\n",
    "            baseline = np.mean(EPpower[f,t])\n",
    "            activity = EPpower[f,:]\n",
    "            powerdB[ep,f,:] = 10*np.log10(activity/baseline)\n",
    "    return powerdB\n",
    "\n",
    "# newEPPower = tfr_morlet(epochs[clas], freqs = frequencies, n_cycles = frequencies/2, return_itc= False, average=False)\n",
    "powerEpdB = {}\n",
    "powerEpdB[classes[0]] = powerEPdBcalc(classes[0])\n",
    "powerEpdB[classes[1]] = powerEPdBcalc(classes[1])\n",
    "# newPower.plot_joint(title = clas, mode = 'mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 3775)\n",
      "(52,)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.vstack((powerEpdB['right'],powerEpdB['foot']))\n",
    "train_data = train_data.reshape(train_data.shape[0],-1)\n",
    "labels = np.concatenate([-np.ones(powerEpdB[classes[0]].shape[0]),\n",
    "                     np.ones(powerEpdB[classes[1]].shape[0])])\n",
    "print(train_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# powerDB.shape #[f,t]\n",
    "# plt.figure()\n",
    "# plt.imshow(powerDB[1,:,:])\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LDA()\n",
    "cv = ShuffleSplit(10, test_size = 0.2, random_state=1)\n",
    "cv_split = cv.split(train_data, labels)\n",
    "scaler = MinMaxScaler((0,1))\n",
    "train_data = scaler.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('scatter', scat_obj), ('clf', clf)])\n",
    "# scores = cross_val_score(pipe, epochs_train_data, labels, cv = cv, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5181818181818181"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfreq = epochs.info['sfreq']\n",
    "w_length = int(sfreq * 0.05)   # running classifier: window length\n",
    "w_step = int(sfreq * 0.01)  # running classifier: window step size\n",
    "w_start = np.arange(0, train_data.shape[1] - w_length, w_step)\n",
    "\n",
    "scores_windows = []\n",
    "\n",
    "for train_idx, test_idx in cv_split:\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "    X_train = train_data[train_idx]\n",
    "    X_test = train_data[test_idx]\n",
    "\n",
    "    # fit classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # running classifier: test classifier on sliding window\n",
    "    score_this_window = []\n",
    "    # for n in w_start:\n",
    "    #     X_test = (train_data[test_idx]) #[:, n:(n + w_length)])\n",
    "    scores_windows.append(clf.score(X_test, y_test))\n",
    "    # scores_windows.append(score_this_window)\n",
    "w_times = (w_start + w_length / 2.) / sfreq + epochs.tmin\n",
    "np.mean(scores_windows)\n",
    "# plt.scatter(scores_windows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
